{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankushpandey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ankushpandey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ankushpandey/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ankushpandey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import operator\n",
    "import pickle\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def get_page_text(url):\n",
    "    return BeautifulSoup(requests.get(url).text, \"html.parser\").get_text()\n",
    "\n",
    "def get_page_links(url):\n",
    "    links = []\n",
    "    for item in BeautifulSoup(requests.get(url).text, \"html.parser\").find_all('a'):\n",
    "        try:\n",
    "            if 'http' in item.get_attribute_list('href')[0]:\n",
    "                links.append(item.get_attribute_list('href')[0])\n",
    "            else:\n",
    "                links.append(url + item.get_attribute_list('href')[0])\n",
    "        except:\n",
    "            pass\n",
    "    return links\n",
    "\n",
    "def read_pdf(loc):\n",
    "    docs = os.listdir(loc)\n",
    "\n",
    "    titles = []\n",
    "    content = []\n",
    "    for file in docs:\n",
    "        pdfFileObj = open(loc + file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        p_cnt = 0\n",
    "        try:\n",
    "            text = pdfReader.getPage(p_cnt).extractText().replace('\\n', ' ').replace('  ', '').lower()\n",
    "            while len(text) < 1000:\n",
    "                print('in loop')\n",
    "                text = pdfReader.getPage(p_cnt).extractText().replace('\\n', ' ').replace('  ', '').lower()\n",
    "                p_cnt += 1\n",
    "            else:\n",
    "                titles.append(loc + file)\n",
    "                content.append(text)\n",
    "        except:\n",
    "            pass\n",
    "    return titles, content\n",
    "\n",
    "def read_pdf_online(links):\n",
    "    titles = []\n",
    "    content = []\n",
    "    for file in links:\n",
    "        try:\n",
    "            remote_file_bytes = io.BytesIO(urllib.request.urlopen(urllib.request.Request(file)).read())\n",
    "            pdfReader = PyPDF2.PdfFileReader(remote_file_bytes)\n",
    "            p_cnt = 0\n",
    "            text = pdfReader.getPage(p_cnt).extractText().replace('\\n', ' ').replace('  ', '').lower()\n",
    "            while len(text) < 1000:\n",
    "                print(f'in loop {len(text)}')\n",
    "                text = pdfReader.getPage(p_cnt).extractText().replace('\\n', ' ').replace('  ', '').lower()\n",
    "                p_cnt += 1\n",
    "            else:\n",
    "                titles.append(file)\n",
    "                content.append(text)\n",
    "        except:\n",
    "            text = get_page_text(file).replace('\\n', ' ').replace('  ', '').lower()\n",
    "            while len(text) < 1000:\n",
    "                print('in loop')\n",
    "                text = get_page_text(file).replace('\\n', ' ').replace('  ', '').lower()\n",
    "            else:\n",
    "                titles.append(file)\n",
    "                content.append(text)\n",
    "    return titles, content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_news):\n",
    "    df_news['content']=[entry.lower() for entry in df_news['content']]\n",
    "    df_news.content =df_news.content.replace(to_replace='from:(.*\\n)',value='',regex=True) #remove from to email \n",
    "    df_news.content =df_news.content.replace(to_replace='lines:(.*\\n)',value='',regex=True)\n",
    "    df_news.content =df_news.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except\n",
    "    df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)\n",
    "    df_news.content =df_news.content.replace(to_replace='\\s+',value=' ',regex=True)    #remove new line\n",
    "    df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space\n",
    "    df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace\n",
    "    df_news.Subject =df_news.Subject.replace(to_replace='Re:',value='',regex=True)\n",
    "    df_news.Subject =df_news.Subject.replace(to_replace='[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True)\n",
    "    df_news.Subject =df_news.Subject.replace(to_replace='\\s+',value=' ',regex=True)    #remove new line\n",
    "    df_news.Subject =df_news.Subject.replace(to_replace='  ',value='',regex=True)    #remove double white space\n",
    "    df_news.Subject =df_news.Subject.apply(lambda x:x.strip())\n",
    "\n",
    "    for i,sen in enumerate(df_news.content):\n",
    "        if len(sen.strip()) ==0:\n",
    "            print(str(i))\n",
    "            df_news=df_news.drop(str(i),axis=0).reset_index().drop('index',axis=1)\n",
    "\n",
    "    return df_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenise(df_news):\n",
    "    df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]\n",
    "    return df_news\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLemmatizer(data):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    file_clean_k =pd.DataFrame()\n",
    "    for index,entry in enumerate(data):\n",
    "        \n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "            # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "    return file_clean_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_more(df_clean):\n",
    "    df_clean=df_clean.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "    df_clean=df_clean.replace(to_replace =\"'\", value = '', regex = True)\n",
    "    df_clean=df_clean.replace(to_replace =\" \", value = '', regex = True)\n",
    "    df_clean=df_clean.replace(to_replace ='\\]', value = '', regex = True)\n",
    "\n",
    "    return df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "\n",
    "def create_vocabulary(df_news):\n",
    "    ## Create Vocabulary\n",
    "    vocabulary = set()\n",
    "\n",
    "    for doc in df_news.Clean_Keyword:\n",
    "        vocabulary.update(doc.split(','))\n",
    "\n",
    "    vocabulary = list(vocabulary)\n",
    "    return vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vector(vocabulary, df_news):\n",
    "    # Intializating the tfIdf model\n",
    "    tfidf = TfidfVectorizer(vocabulary=vocabulary,dtype=np.float32)\n",
    "\n",
    "    # Fit the TfIdf model\n",
    "    tfidf.fit(df_news.Clean_Keyword)\n",
    "\n",
    "    # Transform the TfIdf model\n",
    "    tfidf_tran=tfidf.transform(df_news.Clean_Keyword)\n",
    "    ### Save model\n",
    "    with open('tfid.pkl','wb') as handle:\n",
    "        pickle.dump(tfidf_tran, handle)\n",
    "    \n",
    "    return tfidf, tfidf_tran\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_localrepo():\n",
    "    mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"ap166@jmail\"\n",
    "    )\n",
    "\n",
    "    print(mydb)\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('use docsearch;')\n",
    "\n",
    "    # cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('select * from datastorage;')\n",
    "\n",
    "    extracted_items = list(cursor)\n",
    "\n",
    "    df_news = pd.DataFrame()\n",
    "    for a,b,c in extracted_items:\n",
    "        if b == 3:\n",
    "            loc = c\n",
    "            titles, content = read_pdf(loc)\n",
    "\n",
    "            df_temp = pd.DataFrame()\n",
    "            df_temp['Subject'] = titles\n",
    "            df_temp['content'] = content\n",
    "\n",
    "            df_news = pd.concat([df_news, df_temp])\n",
    "\n",
    "    display(df_news)\n",
    "\n",
    "    df_news = preprocess(df_news)\n",
    "\n",
    "    df_news = tokenise(df_news)\n",
    "    \n",
    "    df_clean = wordLemmatizer(df_news['Word tokenize'])#[0:10])\n",
    "\n",
    "    df_clean = clean_more(df_clean)\n",
    "\n",
    "    df_news.insert(loc=3, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())\n",
    "\n",
    "    df_news_save = df_news\n",
    "    df_news_save = df_news_save.drop(['Word tokenize','Clean_Keyword'],axis=1)\n",
    "    df_news_save.to_csv(\"df_news_index_local.csv\", index=False, header=True)\n",
    "\n",
    "    vocabulary = create_vocabulary(df_news)\n",
    "\n",
    "    tfidf, tfidf_tran = tfidf_vector(vocabulary, df_news)\n",
    "\n",
    "    with open(\"vocabulary_localrepo.txt\", \"w\") as file:\n",
    "        file.write(str(vocabulary))\n",
    "\n",
    "    with open('tfid_localrepo.pkl','wb') as handle:\n",
    "        pickle.dump(tfidf_tran, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x16a177520>\n",
      "in loop\n",
      "in loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/1814-6164-3-PB.pdf</td>\n",
       "      <td>original article meiting chen1*, zhao wang 1*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/WHO list of priority medical devices for ...</td>\n",
       "      <td>!\"#$%&amp;'($)*$+,&amp;),&amp;(-$./0&amp;12%$0/3&amp;1/'$*),$1241/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/1801-6137-2-PB.pdf</td>\n",
       "      <td>original article raltitrexed as a synergistic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/1860-6269-2-PB.pdf</td>\n",
       "      <td>original article phase ii study of apatinib in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/1856-6261-2-PB.pdf</td>\n",
       "      <td>reviewcurrent research on circular rnas and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data/1837-6226-2-PB.pdf</td>\n",
       "      <td>original article vhh212 nanobody targeting the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data/1882-6312-2-PB.pdf</td>\n",
       "      <td>origingal article lncrna dpp10-as1 promotes ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data/1799-6133-2-PB.pdf</td>\n",
       "      <td>original article nedd9 promotes cancer stemnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data/1829-6193-2-PB.pdf</td>\n",
       "      <td>orginal articletianqing chu1*, jun lu1*, mingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data/1824-6183-2-PB.pdf</td>\n",
       "      <td>original article the combination of chidamide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data/1874-6297-2-PB.pdf</td>\n",
       "      <td>original article evgeniya v. dolgova 1, oleg m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data/1830-6195-2-PB.pdf</td>\n",
       "      <td>original article heat shock protein 90 promote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data/1838-6229-2-PB.pdf</td>\n",
       "      <td>original article a novel recurrence-associated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data/1857-6263-2-PB.pdf</td>\n",
       "      <td>original article development and validation of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data/1858-6265-2-PB.pdf</td>\n",
       "      <td>original article cd44v8-10 is a marker for mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data/1807-6150-2-PB.pdf</td>\n",
       "      <td>original article nanlin hu1, yiran si1, jian y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data/1854-6259-2-PB.pdf</td>\n",
       "      <td>original article breast cancer incidence and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data/1808-6152-2-PB.pdf</td>\n",
       "      <td>original articlewinnie yeo 1,2, leung li1, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data/1881-6310-2-PB.pdf</td>\n",
       "      <td>reviewstrategies to enhance monoclonal antibod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data/1847-6249-2-PB.pdf</td>\n",
       "      <td>original article epstein-barr virus dna loads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data/1859-6267-2-PB.pdf</td>\n",
       "      <td>original article juxian sun1*, jiayi wu2,3*, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data/1823-6181-2-PB.pdf</td>\n",
       "      <td>original article transcription factor-based ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data/1849-6253-2-PB.pdf</td>\n",
       "      <td>orginal article complement c3a activates osteo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data/1819-6173-2-PB.pdf</td>\n",
       "      <td>original article fgfr/rack1 interacts with mdm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Subject  \\\n",
       "0                             Data/1814-6164-3-PB.pdf   \n",
       "1   Data/WHO list of priority medical devices for ...   \n",
       "2                             Data/1801-6137-2-PB.pdf   \n",
       "3                             Data/1860-6269-2-PB.pdf   \n",
       "4                             Data/1856-6261-2-PB.pdf   \n",
       "5                             Data/1837-6226-2-PB.pdf   \n",
       "6                             Data/1882-6312-2-PB.pdf   \n",
       "7                             Data/1799-6133-2-PB.pdf   \n",
       "8                             Data/1829-6193-2-PB.pdf   \n",
       "9                             Data/1824-6183-2-PB.pdf   \n",
       "10                            Data/1874-6297-2-PB.pdf   \n",
       "11                            Data/1830-6195-2-PB.pdf   \n",
       "12                            Data/1838-6229-2-PB.pdf   \n",
       "13                            Data/1857-6263-2-PB.pdf   \n",
       "14                            Data/1858-6265-2-PB.pdf   \n",
       "15                            Data/1807-6150-2-PB.pdf   \n",
       "16                            Data/1854-6259-2-PB.pdf   \n",
       "17                            Data/1808-6152-2-PB.pdf   \n",
       "18                            Data/1881-6310-2-PB.pdf   \n",
       "19                            Data/1847-6249-2-PB.pdf   \n",
       "20                            Data/1859-6267-2-PB.pdf   \n",
       "21                            Data/1823-6181-2-PB.pdf   \n",
       "22                            Data/1849-6253-2-PB.pdf   \n",
       "23                            Data/1819-6173-2-PB.pdf   \n",
       "\n",
       "                                              content  \n",
       "0   original article meiting chen1*, zhao wang 1*,...  \n",
       "1   !\"#$%&'($)*$+,&),&(-$./0&12%$0/3&1/'$*),$1241/...  \n",
       "2   original article raltitrexed as a synergistic ...  \n",
       "3   original article phase ii study of apatinib in...  \n",
       "4   reviewcurrent research on circular rnas and th...  \n",
       "5   original article vhh212 nanobody targeting the...  \n",
       "6   origingal article lncrna dpp10-as1 promotes ma...  \n",
       "7   original article nedd9 promotes cancer stemnes...  \n",
       "8   orginal articletianqing chu1*, jun lu1*, mingh...  \n",
       "9   original article the combination of chidamide ...  \n",
       "10  original article evgeniya v. dolgova 1, oleg m...  \n",
       "11  original article heat shock protein 90 promote...  \n",
       "12  original article a novel recurrence-associated...  \n",
       "13  original article development and validation of...  \n",
       "14  original article cd44v8-10 is a marker for mal...  \n",
       "15  original article nanlin hu1, yiran si1, jian y...  \n",
       "16  original article breast cancer incidence and m...  \n",
       "17  original articlewinnie yeo 1,2, leung li1, tho...  \n",
       "18  reviewstrategies to enhance monoclonal antibod...  \n",
       "19  original article epstein-barr virus dna loads ...  \n",
       "20  original article juxian sun1*, jiayi wu2,3*, c...  \n",
       "21  original article transcription factor-based ge...  \n",
       "22  orginal article complement c3a activates osteo...  \n",
       "23  original article fgfr/rack1 interacts with mdm...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_model_localrepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_remoterepo():\n",
    "    mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"ap166@jmail\"\n",
    "    )\n",
    "\n",
    "    print(mydb)\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('use docsearch;')\n",
    "\n",
    "    #cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('select * from datastorage;')\n",
    "\n",
    "    extracted_items = list(cursor)\n",
    "\n",
    "    df_news = pd.DataFrame()\n",
    "    for a,b,c in extracted_items:\n",
    "        if b == 4:\n",
    "            loc = c\n",
    "            titles, content = read_pdf(loc)\n",
    "\n",
    "            df_temp = pd.DataFrame()\n",
    "            df_temp['Subject'] = titles\n",
    "            df_temp['content'] = content\n",
    "\n",
    "            df_news = pd.concat([df_news, df_temp])\n",
    "\n",
    "    display(df_news)\n",
    "\n",
    "    df_news = preprocess(df_news)\n",
    "\n",
    "    df_news = tokenise(df_news)\n",
    "    \n",
    "    df_clean = wordLemmatizer(df_news['Word tokenize'])#[0:10])\n",
    "\n",
    "    df_clean = clean_more(df_clean)\n",
    "\n",
    "    df_news.insert(loc=3, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())\n",
    "\n",
    "    df_news_save = df_news\n",
    "    df_news_save = df_news_save.drop(['Word tokenize','Clean_Keyword'],axis=1)\n",
    "    df_news_save.to_csv(\"df_news_index_remote.csv\", index=False, header=True)\n",
    "\n",
    "    vocabulary = create_vocabulary(df_news)\n",
    "\n",
    "    tfidf, tfidf_tran = tfidf_vector(vocabulary, df_news)\n",
    "\n",
    "    with open(\"vocabulary_remoterepo.txt\", \"w\") as file:\n",
    "        file.write(str(vocabulary))\n",
    "\n",
    "    with open('tfid_remoterepo.pkl','wb') as handle:\n",
    "        pickle.dump(tfidf_tran, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x16a182e20>\n",
      "in loop\n",
      "in loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/1814-6164-3-PB.pdf</td>\n",
       "      <td>original article meiting chen1*, zhao wang 1*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/WHO list of priority medical devices for ...</td>\n",
       "      <td>!\"#$%&amp;'($)*$+,&amp;),&amp;(-$./0&amp;12%$0/3&amp;1/'$*),$1241/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/1801-6137-2-PB.pdf</td>\n",
       "      <td>original article raltitrexed as a synergistic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/1860-6269-2-PB.pdf</td>\n",
       "      <td>original article phase ii study of apatinib in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/1856-6261-2-PB.pdf</td>\n",
       "      <td>reviewcurrent research on circular rnas and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data/1837-6226-2-PB.pdf</td>\n",
       "      <td>original article vhh212 nanobody targeting the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data/1882-6312-2-PB.pdf</td>\n",
       "      <td>origingal article lncrna dpp10-as1 promotes ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data/1799-6133-2-PB.pdf</td>\n",
       "      <td>original article nedd9 promotes cancer stemnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data/1829-6193-2-PB.pdf</td>\n",
       "      <td>orginal articletianqing chu1*, jun lu1*, mingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data/1824-6183-2-PB.pdf</td>\n",
       "      <td>original article the combination of chidamide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data/1874-6297-2-PB.pdf</td>\n",
       "      <td>original article evgeniya v. dolgova 1, oleg m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data/1830-6195-2-PB.pdf</td>\n",
       "      <td>original article heat shock protein 90 promote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data/1838-6229-2-PB.pdf</td>\n",
       "      <td>original article a novel recurrence-associated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data/1857-6263-2-PB.pdf</td>\n",
       "      <td>original article development and validation of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data/1858-6265-2-PB.pdf</td>\n",
       "      <td>original article cd44v8-10 is a marker for mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data/1807-6150-2-PB.pdf</td>\n",
       "      <td>original article nanlin hu1, yiran si1, jian y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data/1854-6259-2-PB.pdf</td>\n",
       "      <td>original article breast cancer incidence and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data/1808-6152-2-PB.pdf</td>\n",
       "      <td>original articlewinnie yeo 1,2, leung li1, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data/1881-6310-2-PB.pdf</td>\n",
       "      <td>reviewstrategies to enhance monoclonal antibod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data/1847-6249-2-PB.pdf</td>\n",
       "      <td>original article epstein-barr virus dna loads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data/1859-6267-2-PB.pdf</td>\n",
       "      <td>original article juxian sun1*, jiayi wu2,3*, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data/1823-6181-2-PB.pdf</td>\n",
       "      <td>original article transcription factor-based ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data/1849-6253-2-PB.pdf</td>\n",
       "      <td>orginal article complement c3a activates osteo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data/1819-6173-2-PB.pdf</td>\n",
       "      <td>original article fgfr/rack1 interacts with mdm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Subject  \\\n",
       "0                             Data/1814-6164-3-PB.pdf   \n",
       "1   Data/WHO list of priority medical devices for ...   \n",
       "2                             Data/1801-6137-2-PB.pdf   \n",
       "3                             Data/1860-6269-2-PB.pdf   \n",
       "4                             Data/1856-6261-2-PB.pdf   \n",
       "5                             Data/1837-6226-2-PB.pdf   \n",
       "6                             Data/1882-6312-2-PB.pdf   \n",
       "7                             Data/1799-6133-2-PB.pdf   \n",
       "8                             Data/1829-6193-2-PB.pdf   \n",
       "9                             Data/1824-6183-2-PB.pdf   \n",
       "10                            Data/1874-6297-2-PB.pdf   \n",
       "11                            Data/1830-6195-2-PB.pdf   \n",
       "12                            Data/1838-6229-2-PB.pdf   \n",
       "13                            Data/1857-6263-2-PB.pdf   \n",
       "14                            Data/1858-6265-2-PB.pdf   \n",
       "15                            Data/1807-6150-2-PB.pdf   \n",
       "16                            Data/1854-6259-2-PB.pdf   \n",
       "17                            Data/1808-6152-2-PB.pdf   \n",
       "18                            Data/1881-6310-2-PB.pdf   \n",
       "19                            Data/1847-6249-2-PB.pdf   \n",
       "20                            Data/1859-6267-2-PB.pdf   \n",
       "21                            Data/1823-6181-2-PB.pdf   \n",
       "22                            Data/1849-6253-2-PB.pdf   \n",
       "23                            Data/1819-6173-2-PB.pdf   \n",
       "\n",
       "                                              content  \n",
       "0   original article meiting chen1*, zhao wang 1*,...  \n",
       "1   !\"#$%&'($)*$+,&),&(-$./0&12%$0/3&1/'$*),$1241/...  \n",
       "2   original article raltitrexed as a synergistic ...  \n",
       "3   original article phase ii study of apatinib in...  \n",
       "4   reviewcurrent research on circular rnas and th...  \n",
       "5   original article vhh212 nanobody targeting the...  \n",
       "6   origingal article lncrna dpp10-as1 promotes ma...  \n",
       "7   original article nedd9 promotes cancer stemnes...  \n",
       "8   orginal articletianqing chu1*, jun lu1*, mingh...  \n",
       "9   original article the combination of chidamide ...  \n",
       "10  original article evgeniya v. dolgova 1, oleg m...  \n",
       "11  original article heat shock protein 90 promote...  \n",
       "12  original article a novel recurrence-associated...  \n",
       "13  original article development and validation of...  \n",
       "14  original article cd44v8-10 is a marker for mal...  \n",
       "15  original article nanlin hu1, yiran si1, jian y...  \n",
       "16  original article breast cancer incidence and m...  \n",
       "17  original articlewinnie yeo 1,2, leung li1, tho...  \n",
       "18  reviewstrategies to enhance monoclonal antibod...  \n",
       "19  original article epstein-barr virus dna loads ...  \n",
       "20  original article juxian sun1*, jiayi wu2,3*, c...  \n",
       "21  original article transcription factor-based ge...  \n",
       "22  orginal article complement c3a activates osteo...  \n",
       "23  original article fgfr/rack1 interacts with mdm...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_model_remoterepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_weblinks():\n",
    "    mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"ap166@jmail\"\n",
    "    )\n",
    "\n",
    "    print(mydb)\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('use docsearch;')\n",
    "\n",
    "    #cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('select * from datastorage;')\n",
    "\n",
    "    extracted_items = list(cursor)\n",
    "\n",
    "    links = []\n",
    "    for a,b,c in extracted_items:\n",
    "        if b == 2:\n",
    "            links.append(c)\n",
    "\n",
    "    titles, content = read_pdf_online(links)\n",
    "\n",
    "    df_news = pd.DataFrame()\n",
    "    df_news['Subject'] = titles\n",
    "    df_news['content'] = content\n",
    "\n",
    "    display(df_news)\n",
    "\n",
    "    df_news = preprocess(df_news)\n",
    "\n",
    "    df_news = tokenise(df_news)\n",
    "    \n",
    "    df_clean = wordLemmatizer(df_news['Word tokenize'])#[0:10])\n",
    "\n",
    "    df_clean = clean_more(df_clean)\n",
    "\n",
    "    df_news.insert(loc=3, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())\n",
    "\n",
    "    df_news_save = df_news\n",
    "    df_news_save = df_news_save.drop(['Word tokenize','Clean_Keyword'],axis=1)\n",
    "    df_news_save.to_csv(\"df_news_index_weblinks.csv\", index=False, header=True)\n",
    "\n",
    "    vocabulary = create_vocabulary(df_news)\n",
    "\n",
    "    tfidf, tfidf_tran = tfidf_vector(vocabulary, df_news)\n",
    "\n",
    "    with open(\"vocabulary_weblinks.txt\", \"w\") as file:\n",
    "        file.write(str(vocabulary))\n",
    "\n",
    "    with open('tfid_weblinks.pkl','wb') as handle:\n",
    "        pickle.dump(tfidf_tran, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x16a278a90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.cancerbiomed.org/index.php/cocr/art...</td>\n",
       "      <td>original article heat shock protein 90 promote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.cancerbiomed.org/index.php/cocr/art...</td>\n",
       "      <td>breast cancer incidence and mortality in women...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  \\\n",
       "0  http://www.cancerbiomed.org/index.php/cocr/art...   \n",
       "1  http://www.cancerbiomed.org/index.php/cocr/art...   \n",
       "\n",
       "                                             content  \n",
       "0  original article heat shock protein 90 promote...  \n",
       "1  breast cancer incidence and mortality in women...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_model_weblinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_library():\n",
    "    mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"ap166@jmail\"\n",
    "    )\n",
    "\n",
    "    print(mydb)\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('use docsearch;')\n",
    "\n",
    "    #cursor = mydb.cursor()\n",
    "\n",
    "    cursor.execute('select * from datastorage;')\n",
    "\n",
    "    extracted_items = list(cursor)\n",
    "\n",
    "    df_news = pd.DataFrame()\n",
    "    for a,b,c in extracted_items:\n",
    "        if b == 1:\n",
    "            try:\n",
    "                links = get_page_links(c)[15:20]\n",
    "                print(links)\n",
    "                titles, content = read_pdf_online(links)\n",
    "                df_temp = pd.DataFrame()\n",
    "                df_temp['Subject'] = titles\n",
    "                df_temp['content'] = content\n",
    "\n",
    "                df_news = pd.concat([df_news, df_temp])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "    display(df_news)\n",
    "\n",
    "    df_news = preprocess(df_news)\n",
    "\n",
    "    df_news = tokenise(df_news)\n",
    "    \n",
    "    df_clean = wordLemmatizer(df_news['Word tokenize'])#[0:10])\n",
    "\n",
    "    df_clean = clean_more(df_clean)\n",
    "\n",
    "    df_news.insert(loc=3, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())\n",
    "\n",
    "    df_news_save = df_news\n",
    "    df_news_save = df_news_save.drop(['Word tokenize','Clean_Keyword'],axis=1)\n",
    "    df_news_save.to_csv(\"df_news_index_library.csv\", index=False, header=True)\n",
    "\n",
    "    vocabulary = create_vocabulary(df_news)\n",
    "\n",
    "    tfidf, tfidf_tran = tfidf_vector(vocabulary, df_news)\n",
    "\n",
    "    with open(\"vocabulary_library.txt\", \"w\") as file:\n",
    "        file.write(str(vocabulary))\n",
    "\n",
    "    with open('tfid_library.pkl','wb') as handle:\n",
    "        pickle.dump(tfidf_tran, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x168d73c40>\n",
      "['http://www.sciepub.com/journal/ajcp', 'https://www.ncbi.nlm.nih.gov/labs/pmc/journals/1608/', 'http://www.ncbi.nlm.nih.gov/pubmed?term=%22Am%20J%20Cancer%20Res%22%5BJour%5D%20AND%20%22loattrfree%20full%20text%22%5Bsb%5D', 'http://ivyunion.org/index.php/ajcs', 'https://www.jstage.jst.go.jp/browse/acrt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.sciepub.com/journal/ajcp</td>\n",
       "      <td>\\r \\tamerican journal of cancer prevention\\r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/labs/pmc/journals...</td>\n",
       "      <td>archive of \"american journal of cancer resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pubmed?term=%22Am%...</td>\n",
       "      <td>\"am j cancer res\"[jour] and \"loattrfree full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://ivyunion.org/index.php/ajcs</td>\n",
       "      <td>american journal of cancer scienceuserusernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.jstage.jst.go.jp/browse/acrt</td>\n",
       "      <td>annals of cancer research and therapy toggle ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  \\\n",
       "0                http://www.sciepub.com/journal/ajcp   \n",
       "1  https://www.ncbi.nlm.nih.gov/labs/pmc/journals...   \n",
       "2  http://www.ncbi.nlm.nih.gov/pubmed?term=%22Am%...   \n",
       "3                 http://ivyunion.org/index.php/ajcs   \n",
       "4           https://www.jstage.jst.go.jp/browse/acrt   \n",
       "\n",
       "                                             content  \n",
       "0   \\r \\tamerican journal of cancer prevention\\r ...  \n",
       "1   archive of \"american journal of cancer resear...  \n",
       "2   \"am j cancer res\"[jour] and \"loattrfree full ...  \n",
       "3   american journal of cancer scienceuserusernam...  \n",
       "4   annals of cancer research and therapy toggle ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_model_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343986f8465ea7f501665066340a6e4a7b639b52c302955116ce941ba04487e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
